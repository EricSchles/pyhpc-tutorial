{
 "metadata": {
  "name": "03_Scaling_Python"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python in HPC\n",
      "=============\n",
      "\n",
      "Supercomputing 2012\n",
      "-------------------\n",
      "\n",
      "Presenters:\n",
      "\n",
      "Andy R. Terrel, PhD  \n",
      "Texas Advanced Computing Center  \n",
      "University of Texas at Austin  \n",
      "\n",
      "Travis Oliphant, PhD  \n",
      "Continuum Analytics\n",
      "\n",
      "Aron Ahmadia, PhD  \n",
      "Supercomputing Laboratory  \n",
      "King Abdullah University of Science and Technoglogy\n",
      "\n",
      "\n",
      "<img src=\"/files/figures/TACC_logo.png\" />\n",
      "<img src=\"/files/figures/continuum.png\" />\n",
      "<img src=\"/files/figures/kaust.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "## Updated Tutorial\n",
      "\n",
      "To get an updated version of this tutorial you can:\n",
      "\n",
      "1) Download a zip or tar ball from the [github SC2012 tag](https://github.com/aterrel/HPCPythonSC2012/tags):\n",
      "\n",
      "    wget --no-check-certificate https://github.com/aterrel/HPCPythonSC2012/zipball/SC2012\n",
      "    wget --no-check-certificate https://github.com/aterrel/HPCPythonSC2012/tarball/SC2012\n",
      "\n",
      "2) Checkout from git\n",
      "\n",
      "    git clone https://github.com/aterrel/HPCPythonSC2012.git"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "## Running the Tutorial\n",
      "\n",
      "This tutorial is actually an interactive worksheet designed to encourage the students to try out the lessons live.  If you are looking at the pdf version, you might download the updated version (see last slide) and try the interactive version.\n",
      "\n",
      "To run the interactive version, you need a good Python environment including (such as [EPD](http://www.enthought.com/products/epd.php)):\n",
      "\n",
      "* IPython version >= 13.0\n",
      "* Numpy version >= 1.5\n",
      "* Scipy\n",
      "* Matplotlib\n",
      "\n",
      "Move to the directory containing the tarball and execute:\n",
      "\n",
      "    $ ipython notebook --pylab=inline"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Acknowledgements\n",
      "-------------------------------------\n",
      "\n",
      "* Much of this tutorial adapts slide material from [William Gropp](http://www.cs.uiuc.edu/~wgropp/), University of Illinois\n",
      "* [mpi4py](http://code.google.com/p/mpi4py/) is a [Cythonized](http://www.cython.org/) wrapper around [MPI](http://www.mpi-forum.org/) originally developed by [Lisandro Dalcin](http://plus.google.com/107621373684536061961/about), CONICET"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "The Free Lunch is Over\n",
      "-------------------------------------\n",
      "\n",
      "![freelunch](/files/figures/free_lunch.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "# The Multiple Forms of Parallelism\n",
      "\n",
      "* **instruction** - multiple program instructions are simultaneously dispatched in a pipeline or to multiple execution units (superscalar)\n",
      "* **data** - the same program instructions are carried out simultaneously on multiple data items (SIMD)\n",
      "* **task** - different program instructions on different data (MIMD)\n",
      "* **collective** - single program, multiple data, not necessarily synchronized at individual operation level (SPMD)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "The Rise of Manycore (TODO - get a better concurrency graphic)\n",
      "-------------------------------------\n",
      "\n",
      "![concurrency](/files/figures/concurrency.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "# Parallel Architectures\n",
      "\n",
      "* All modern supercomputing architectures are composed of distributed memory compute nodes connected over toroidal networks\n",
      "  \n",
      "  - close mapping to physically n-dimensional problems\n",
      "  - efficient algorithms exist for local point-to-point and collective communications across toroids\n",
      "\n",
      "* We expect to see higher dimensional interconnects and power-efficient networks that consume less power when not sending traffic\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "# Parallel Programming Paradigms\n",
      "\n",
      "* a parallel programming paradigm is a specific approach to exploiting parallelism in hardware\n",
      "* many programming paradigms are very tightly coupled to the hardware beneath!\n",
      "  \n",
      " - CUDA assumes large register files, Same Instruction Multiple Thread parallelism, and a mostly flat, structured memory model, matching the underlying GPU hardware\n",
      "\n",
      " - OpenMP exposes loop level parallelism with a fork/join model, assumes the presence of shared memory and atomics\n",
      "\n",
      " - OpenCl tries to generalize CUDA, but still assumes a 'coprocessor' approach, where kernels are shipped from a master core to worker cores\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "The Message Passing Model\n",
      "-------------------------------------\n",
      "\n",
      "* a process is (traditionally) a program counter for instructions and an address space for data\n",
      "* processes may have multiple threads (program counters and associated stacks) sharing a single address space\n",
      "* message passing is for communication among processes, which have separate address spaces\n",
      "* interprocess communication consists of\n",
      "  \n",
      "  - synchronization\n",
      "  - movement of data from one process's address space to another's\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Why MPI?\n",
      "-------------------------------------\n",
      "\n",
      "* **communicators** encapsulate communication spaces for library safety\n",
      "* **datatypes** reduce copying costs and permit heterogeneity\n",
      "* multiple **communication modes** allow more control of memory buffer management\n",
      "* extensive **collective operations** for scalable global communication\n",
      "* **process topologies** permit efficient process placement, user views of process layout\n",
      "* **profiling interface** encourages portable tools\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Communicators\n",
      "-------------------------------------\n",
      "\n",
      "* processes can be collected into **groups**\n",
      "* each message is sent in a **context**, and must be received in the same context\n",
      "* a  **communicator** encapsulates a context for a specific group\n",
      "* a given program may have many communicators with any level of overlap\n",
      "* two initial communicators\n",
      " \n",
      "  - ``MPI_COMM_WORLD`` (all processes)\n",
      "  - ``MPI_COMM_SELF`` (current process)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Datatypes\n",
      "-------------------------------------\n",
      "\n",
      "* the data in a message to send or receive is described by address, count and datatype\n",
      "* a datatype is recursively defined as:\n",
      "\n",
      "  + predefined, corresponding to a data type from the language (e.g., ``MPI_INT``, ``MPI_DOUBLE``)\n",
      "  + a contiguous, strided block, or indexed array of blocks of MPI datatypes\n",
      "  + an arbitrary structure of datatypes\n",
      "\n",
      "* there are MPI functions to construct custom datatypes\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Tags\n",
      "-------------------------------------\n",
      "\n",
      "* messages are sent with an accompanying user-defined integer tag to assist the receiving process in identifying the message\n",
      "* messages can be screened at the receiving end by specifying the expected tag, or not screened by using ``MPI_ANY_TAG`` \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Checkpoint #1\n",
      "-------------------------------------\n",
      "\n",
      "If tags allow us to screen/separate peer-to-peer messages, why do we need communicators?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "MPI Basic (Blocking) Send\n",
      "-------------------------------------\n",
      "\n",
      "C\n",
      "\n",
      "```\n",
      "   int MPI_Send(void* buf, int count, MPI_Datatype type, \n",
      "   int dest, int tag, MPI_Comm comm)\n",
      "```\n",
      "\n",
      "Python (mpi4py)\n",
      "\n",
      "```\n",
      "  Comm.Send(self, buf, int dest=0, int tag=0)\n",
      "  Comm.send(self, obj=None, int dest=0, int tag=0)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "MPI Basic (Blocking) Recv\n",
      "-------------------------------------\n",
      "\n",
      "C\n",
      "\n",
      "```\n",
      "   int MPI_Recv(void* buf, int count, MPI_Datatype type, \n",
      "   int source, int tag, MPI_Comm comm, MPI_Status status)\n",
      "```\n",
      "\n",
      "Python (mpi4py)\n",
      "\n",
      "```\n",
      "  comm.Recv(self, buf, int source=0, int tag=0, \n",
      "  Status status=None)\n",
      "  comm.recv(self, obj=None, int source=0, \n",
      "  int tag=0, Status status=None)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Synchronization\n",
      "-------------------------------------\n",
      "\n",
      "C\n",
      "\n",
      "```\n",
      "   int MPI_Barrier(MPI_Comm comm)\n",
      "```\n",
      "\n",
      "Python (mpi4py)\n",
      "\n",
      "```\n",
      "   comm.Barrier(self)\n",
      "   comm.barrier(self)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Broadcast/Reduce\n",
      "-------------------------------------\n",
      "\n",
      "```\n",
      "   int MPI_Bcast(void *buf, int count, MPI_Datatype type, \n",
      "   int root, MPI_Comm comm)\n",
      "```\n",
      "\n",
      "Python (mpi4py)\n",
      "\n",
      "``` \n",
      "  comm.Bcast(self, buf, int root=0)\n",
      "  comm.bcast(self, obj=None, int root=0)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "collective data movement\n",
      "-------------------------------------\n",
      "\n",
      "![broadcast_scatter_gather](/files/figures/broadcast_scatter_gather.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "collective data movement\n",
      "-------------------------------------\n",
      "\n",
      "![allgather_alltoall](/files/figures/allgather_alltoall.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "collective data movement\n",
      "-------------------------------------\n",
      "\n",
      "![reduce_scan](/files/figures/reduce_scan.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Timing and Profiling\n",
      "-------------------------------------\n",
      "\n",
      "the elapsed (wall-clock) time between two points in an MPI program can be computed using MPI_Wtime:\n",
      "\n",
      "```\n",
      "      t1 = MPI.Wtime()\n",
      "      t2 = MPI.Wtime()\n",
      "      print(\"time elapsed is: %e\\n\" % (t2-t1))\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}