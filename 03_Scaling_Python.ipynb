{
 "metadata": {
  "name": "03_Scaling_Python"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Python in HPC\n",
      "=============\n",
      "\n",
      "Supercomputing 2012\n",
      "-------------------\n",
      "\n",
      "Presenters:\n",
      "\n",
      "Andy R. Terrel, PhD  \n",
      "Texas Advanced Computing Center  \n",
      "University of Texas at Austin  \n",
      "\n",
      "Travis Oliphant, PhD  \n",
      "Continuum Analytics\n",
      "\n",
      "Aron Ahmadia, PhD  \n",
      "Supercomputing Laboratory  \n",
      "King Abdullah University of Science and Technoglogy\n",
      "\n",
      "[![Creative Commons License](http://i.creativecommons.org/l/by/3.0/88x31.png)](http://creativecommons.org/licenses/by/3.0/deed.en_US)  \n",
      "\n",
      "![TACC Logo](/files/figures/TACC_logo.png)\n",
      "![Continuum Logo](/files/figures/continuum.png)\n",
      "![KAUST Logo](/files/figures/kaust.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "## Updated Tutorial\n",
      "\n",
      "These presentation materials are being continuously updated as we refine and improve our demonstrats.  To get the latest version of this tutorial you can:\n",
      "\n",
      "1) Download a zip or tar ball from the [github SC2012 tag](https://github.com/aterrel/HPCPythonSC2012/tags):\n",
      "\n",
      "    wget --no-check-certificate https://github.com/aterrel/HPCPythonSC2012/zipball/SC2012\n",
      "    wget --no-check-certificate https://github.com/aterrel/HPCPythonSC2012/tarball/SC2012\n",
      "\n",
      "2) Checkout from git\n",
      "\n",
      "    git clone https://github.com/aterrel/HPCPythonSC2012.git\n",
      "\n",
      "3) View the html version on nbviewer: http://nbviewer.ipython.org/urls/raw.github.com/aterrel/HPCPythonSC2012/master/01_Introducing_Python.ipynb\n",
      "\n",
      "4) As a last resort, head to https://github.com/aterrel/HPCPythonSC2012 for updated instructions (see the README at the bottom of the page).\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "## Interacting with the Tutorial Slides\n",
      "\n",
      "This tutorial is an interactive worksheet designed to encourage you to try out the lessons during the demonstration.  If you are looking at the pdf version, we encourage you to download the updated version (see previous slide) and try the interactive version.\n",
      "\n",
      "To run the interactive version, you need a good Python environment including: \n",
      "\n",
      "* IPython version >= 13.0\n",
      "* Numpy version >= 1.5\n",
      "* Scipy\n",
      "* Matplotlib\n",
      "\n",
      "Move to the directory containing the tarball and execute:\n",
      "\n",
      "    $ ipython notebook --pylab=inline\n",
      "\n",
      "We heartily endorse the [Free Enthought Python Distribution](http://www.enthought.com/products/epd_free.php)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Acknowledgements\n",
      "-------------------------------------\n",
      "\n",
      "* Much of this tutorial adapts slide material from [William Gropp](http://www.cs.uiuc.edu/~wgropp/), University of Illinois\n",
      "* [mpi4py](http://code.google.com/p/mpi4py/) is a [Cythonized](http://www.cython.org/) wrapper around [MPI](http://www.mpi-forum.org/) originally developed by [Lisandro Dalcin](http://plus.google.com/107621373684536061961/about), CONICET"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "The Free Lunch is Over\n",
      "-------------------------------------\n",
      "\n",
      "![freelunch](/files/figures/free_lunch.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "# The Multiple Forms of Parallelism\n",
      "\n",
      "* **instruction** - multiple program instructions are simultaneously dispatched in a pipeline or to multiple execution units (superscalar)\n",
      "* **data** - the same program instructions are carried out simultaneously on multiple data items (SIMD)\n",
      "* **task** - different program instructions on different data (MIMD)\n",
      "* **collective** - single program, multiple data, not necessarily synchronized at individual operation level (SPMD)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "The Rise of Manycore\n",
      "-------------------------------------\n",
      "\n",
      "![concurrency](/files/figures/concurrency_2.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "# Parallel Architectures\n",
      "\n",
      "* All modern supercomputing architectures are composed of distributed memory compute nodes connected over toroidal networks\n",
      "  \n",
      "  - close mapping to physically n-dimensional problems\n",
      "  - efficient algorithms exist for local point-to-point and collective communications across toroids\n",
      "\n",
      "* We expect to see higher dimensional interconnects and power-efficient networks that consume less power when not sending traffic\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "# Parallel Programming Paradigms\n",
      "\n",
      "* a parallel programming paradigm is a specific approach to exploiting parallelism in hardware\n",
      "* many programming paradigms are very tightly coupled to the hardware beneath!\n",
      "  \n",
      " - CUDA assumes large register files, Same Instruction Multiple Thread parallelism, and a mostly flat, structured memory model, matching the underlying GPU hardware\n",
      "\n",
      " - OpenMP exposes loop level parallelism with a fork/join model, assumes the presence of shared memory and atomics\n",
      "\n",
      " - OpenCl tries to generalize CUDA, but still assumes a 'coprocessor' approach, where kernels are shipped from a master core to worker cores\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "The Message Passing Model\n",
      "-------------------------------------\n",
      "\n",
      "* a process is (traditionally) a program counter for instructions and an address space for data\n",
      "* processes may have multiple threads (program counters and associated stacks) sharing a single address space\n",
      "* message passing is for communication among processes, which have separate address spaces\n",
      "* interprocess communication consists of\n",
      "  \n",
      "  - synchronization\n",
      "  - movement of data from one process's address space to another's\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Why MPI?\n",
      "-------------------------------------\n",
      "\n",
      "* **communicators** encapsulate communication spaces for library safety\n",
      "* **datatypes** reduce copying costs and permit heterogeneity\n",
      "* multiple **communication modes** allow more control of memory buffer management\n",
      "* extensive **collective operations** for scalable global communication\n",
      "* **process topologies** permit efficient process placement, user views of process layout\n",
      "* **profiling interface** encourages portable tools\n",
      "\n",
      "It Scales!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Python + MPI Does It Scale?\n",
      "-------------------------------------\n",
      "\n",
      "* This and next slide courtesy the **PyClaw** project: http://numerics.kaust.edu.sa/pyclaw/\n",
      "* See also **GPAW**: https://wiki.fysik.dtu.dk/gpaw/\n",
      "\n",
      "![concurrency](/files/figures/does_it_scale.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "The Scalability of Python + MPI\n",
      "-------------------------------------\n",
      "\n",
      "![concurrency](/files/figures/euler_weak_scaling.png)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "MPI - Quick Review\n",
      "-------------------------------------\n",
      "\n",
      "* processes can be collected into **groups**\n",
      "* each message is sent in a **context**, and must be received in the same context\n",
      "* a  **communicator** encapsulates a context for a specific group\n",
      "* a given program may have many communicators with any level of overlap\n",
      "* two initial communicators\n",
      " \n",
      "  - ``MPI_COMM_WORLD`` (all processes)\n",
      "  - ``MPI_COMM_SELF`` (current process)\n",
      "\n",
      "\n",
      "Communicators\n",
      "-------------------------------------\n",
      "\n",
      "* processes can be collected into **groups**\n",
      "* each message is sent in a **context**, and must be received in the same context\n",
      "* a  **communicator** encapsulates a context for a specific group\n",
      "* a given program may have many communicators with any level of overlap\n",
      "* two initial communicators\n",
      " \n",
      "  - ``MPI_COMM_WORLD`` (all processes)\n",
      "  - ``MPI_COMM_SELF`` (current process)\n",
      "\n",
      "\n",
      "Datatypes\n",
      "-------------------------------------\n",
      "\n",
      "* the data in a message to send or receive is described by address, count and datatype\n",
      "* a datatype is recursively defined as:\n",
      "\n",
      "  + predefined, corresponding to a data type from the language (e.g., ``MPI_INT``, ``MPI_DOUBLE``)\n",
      "  + a contiguous, strided block, or indexed array of blocks of MPI datatypes\n",
      "  + an arbitrary structure of datatypes\n",
      "\n",
      "* there are MPI functions to construct custom datatypes\n",
      "\n",
      "\n",
      "Tags\n",
      "-------------------------------------\n",
      "\n",
      "* messages are sent with an accompanying user-defined integer tag to assist the receiving process in identifying the message\n",
      "* messages can be screened at the receiving end by specifying the expected tag, or not screened by using ``MPI_ANY_TAG`` \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "MPI Basic (Blocking) Send\n",
      "-------------------------------------\n",
      "\n",
      "```\n",
      "   int MPI_Send(void* buf, int count, MPI_Datatype type, \n",
      "   int dest, int tag, MPI_Comm comm)\n",
      "```\n",
      "\n",
      "Python (mpi4py)\n",
      "\n",
      "```\n",
      "  Comm.Send(self, buf, int dest=0, int tag=0)\n",
      "  Comm.send(self, obj=None, int dest=0, int tag=0)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "MPI Basic (Blocking) Recv\n",
      "-------------------------------------\n",
      "\n",
      "```\n",
      "   int MPI_Recv(void* buf, int count, MPI_Datatype type, \n",
      "   int source, int tag, MPI_Comm comm, MPI_Status status)\n",
      "```\n",
      "\n",
      "Python (mpi4py)\n",
      "\n",
      "```\n",
      "  comm.Recv(self, buf, int source=0, int tag=0, \n",
      "  Status status=None)\n",
      "  comm.recv(self, obj=None, int source=0, \n",
      "  int tag=0, Status status=None)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Synchronization\n",
      "-------------------------------------\n",
      "\n",
      "```\n",
      "   int MPI_Barrier(MPI_Comm comm)\n",
      "```\n",
      "\n",
      "Python (mpi4py)\n",
      "\n",
      "```\n",
      "   comm.Barrier(self)\n",
      "   comm.barrier(self)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "Timing and Profiling\n",
      "-------------------------------------\n",
      "\n",
      "the elapsed (wall-clock) time between two points in an MPI program can be computed using MPI_Wtime:\n",
      "\n",
      "```\n",
      "      t1 = MPI.Wtime()\n",
      "      t2 = MPI.Wtime()\n",
      "      print(\"time elapsed is: %e\\n\" % (t2-t1))\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "Send/Receive Example (lowercase convenience methods)\n",
      "-------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpi4py import MPI\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "\n",
      "if rank == 0:\n",
      "   data = {'a': 7, 'b': 3.14}\n",
      "   comm.send(data, dest=1, tag=11)\n",
      "elif rank == 1:\n",
      "   data = comm.recv(source=0, tag=11)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "Send/Receive Example (MPI API on numpy)\n",
      "-------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpi4py import MPI\n",
      "import numpy\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "\n",
      "# pass explicit MPI datatypes\n",
      "if rank == 0:\n",
      "   data = numpy.arange(1000, dtype='i')\n",
      "   comm.Send([data, MPI.INT], dest=1, tag=77)\n",
      "elif rank == 1:\n",
      "   data = numpy.empty(1000, dtype='i')\n",
      "   comm.Recv([data, MPI.INT], source=0, tag=77)\n",
      "\n",
      "# or take advantage of automatic MPI datatype discovery\n",
      "if rank == 0:\n",
      "   data = numpy.arange(100, dtype=numpy.float64)\n",
      "   comm.Send(data, dest=1, tag=13)\n",
      "elif rank == 1:\n",
      "   data = numpy.empty(100, dtype=numpy.float64)\n",
      "   comm.Recv(data, source=0, tag=13)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "Broadcast/Reduce\n",
      "-------------------------------------\n",
      "\n",
      "```\n",
      "   int MPI_Bcast(void *buf, int count, MPI_Datatype type, \n",
      "   int root, MPI_Comm comm)\n",
      "```\n",
      "\n",
      "Python (mpi4py)\n",
      "\n",
      "``` \n",
      "  comm.Bcast(self, buf, int root=0)\n",
      "  comm.bcast(self, obj=None, int root=0)\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "Collective Example\n",
      "-------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpi4py import MPI\n",
      "\n",
      "comm = MPI.COMM_WORLD\n",
      "rank = comm.Get_rank()\n",
      "\n",
      "if rank == 0:\n",
      "   data = {'key1' : [7, 2.72, 2+3j],\n",
      "           'key2' : ( 'abc', 'xyz')}\n",
      "else:\n",
      "   data = None\n",
      "data = comm.bcast(data, root=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "More Comprehensive mpi4py Tutorials\n",
      "----------------------------\n",
      "\n",
      "* basics - http://mpi4py.scipy.org/docs/usrman/tutorial.html\n",
      "* advanced - http://www.bu.edu/pasi/files/2011/01/Lisandro-Dalcin-mpi4py.pdf\n",
      "\n",
      "Interesting Scalable Applications and Tools\n",
      "---------------------------------------------\n",
      "* PyTrilinos - http://trilinos.sandia.gov/packages/pytrilinos/\n",
      "* petsc4py - http://code.google.com/p/petsc4py/\n",
      "* PyClaw - http://numerics.kaust.edu.sa/pyclaw/\n",
      "* GPAW - https://wiki.fysik.dtu.dk/gpaw/\n",
      "\n",
      "--> See Appendix 01 References for more resources\n",
      "----------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}